# Assignment-1
## What is Big Data?
Big data is a term describing information that demands sophisticated processing and analysis because it is high in volume, high in speed, or high in variety. Data alone isn't helpful; analysis of the data is essential for streamlining company procedures. Big data analysis is done by businesses using a variety of methods, including data mining, which brings out patterns in the data. For instance, businesses might use data mining to discover which sales offers will appeal to particular customers. Companies that handle big data properly may make better judgments, provide better products and services to customers, and give better customer service.

## Types of Big Data
1.Structured Data:It is simple to query and analyze structured data since it is well-organized and has a set format. Relational databases and spreadsheets frequently contain it.
  Example: Transaction records, accounting, stock market.

2.Unstructured Data: Unstructured data is difficult to examine using conventional techniques because it lacks a preset framework. It consists of audio, video, text, and image files.
 Example: photos, videos, recorded messages, satellite images.

3.Semi-Structured Data: Although semi-structured data is somewhat organized, it does not follow the rigid rules of structured data. To give some sort of organizing, it frequently makes use of tags or meta data. 
 Example: XML data, JSON files, NoSQL Database.

 ## 6 ‘V’s of Big Data 

The 6 V’s of Big Data are volume, velocity, variety,veracity,variability, value

VOLUME: As the name says it all volume means huge, Big Data involves huge amounts of data which are often measured in terabytes, petabytes or even bytes.

VELOCITY: Data is generated and gathered in real-time or very close to real-time at an exceedingly high rate. It may be difficult to quickly process and analyze the data given this pace.

VARIETY: Big Data is available in a variety of formats and types, including unstructured data (such as text, photos, and videos), semi-structured data (such as XML or JSON), and structured data (like databases).

VERACITY: This factor has to do with how reliable and high-quality the data is. Big Data must be cleaned and validated because it may contain noisy or missing information.

VARIABILITY: It might be difficult to smoothly integrate and evaluate data since it can be inconsistent or change in format and organization over time.

VALUE: Big Data should ultimately offer organizations useful insights and advantages. Better decision-making and greater business outcomes may result from the extraction of insightful information from this data.

source:GeeksforGeeks. (2023). 6V s of Big Data. GeeksforGeeks. https://www.geeksforgeeks.org/5-vs-of-big-data/

## Phases of Big Data analysis

Big Data analysis frequently takes numerous stages in order to convert raw data into insightful knowledge and useful information.

Data gathering and acquisition:
Data is gathered in this first phase from a variety of sources, such as databases, sensors, online applications, social media platforms, and more.
Data can be structured, semi-structured, or unstructured, and it can be streamed in real-time or regularly gathered.

Ingestion of data:
A centralized storage system, such as a data warehouse, data lake, or distributed file system, must be used to store the data gathered from numerous sources.
To guarantee consistency and quality, data may be cleansed, processed, and formatted during ingestion.

Data cleaning and preprocessing: 
Raw data frequently has errors, missing numbers, and discrepancies. Data is cleaned, processed, and ready for analysis at this step.
It is possible to use methods like data imputation, outlier detection, and data standardization.

Data Management and Storage:
Data is effectively managed and stored in a format that is suitable for analysis. Choosing the proper storage method and data structure is a part of this process.
Many times, cloud-based storage, NoSQL databases, and the Hadoop Distributed File System (HDFS) are used.

Data exploration and visualization: 
To comprehend the properties, trends, and linkages of the data, analysts and data scientists study it.
Insights can be visualized using graphs, charts, and dashboards created using data visualization tools and techniques.

Data Transformation and Feature Engineering:
Data can be modified to produce new features or variables that are more useful for analysis.
Data aggregation, dimensional reduction, and the creation of computed fields are all examples of feature engineering.

Data Analysis and Modeling:
Various statistical, machine learning, and deep learning models are used to the prepared data to extract insights and generate predictions during this phase.
For analysis, analysts may employ tools such as Python, R, or specific Big Data platforms such as Apache Spark.

Model Evaluation and Validation: 
To examine the performance and accuracy of models, relevant metrics are used.
To ensure model reliability, cross-validation and testing on unknown data are popular procedures.

Data Interpretation and Insight Generation: 
The analytic results are interpreted in order to produce relevant insights and actionable recommendations.
Domain knowledge is essential for comprehending the consequences of the findings.

Reporting and Visualization:
Insights and discoveries are documented and presented in reports, dashboards, or presentations.
Visualizations are used to effectively communicate outcomes to stakeholders.

Deployment and Integration:
If the study results in the construction of operational models or systems, they are deployed and integrated into the current infrastructure.
This stage ensures that the findings are put to use.

Continuous Monitoring and Maintenance: 
To ensure data quality, model performance, and system stability, Big Data systems require ongoing monitoring.
As new data becomes available or business objectives change, updates and refinements may be required.

source: 8 steps in the data life Cycle | HBS Online. (2021, February 2). Business Insights Blog. https://online.hbs.edu/blog/post/data-life-cycle

## Challenges in Big Data

1. Dealing with enormous amounts of data:
It's in the name big data is massive. Most businesses are expanding the amount of data they acquire on a daily basis. Many corporate leaders are concerned that the storage capacity provided by a standard data center may eventually be insufficient. 43% of IT decision-makers in the technology sector are concerned about this data inflow overwhelming their infrastructure.

2. Combining data from many sources:
Another difficulty for corporations is the data itself. There is a lot, but it is also diversified because information can originate from various sources. A company may have analytics  data from many websites, social media sharing data, user information from CRM software, email data, and more. None of these data is structured the same way, yet it may need to be combined and reconciled in order to obtain the essential insights and make reports.

3. Ensuring the accuracy of data:
Big data analytics and machine learning procedures rely on clean, correct data to create valid insights and predictions. The results may not be what you expect if the data is damaged or incomplete. However, when the sources, types, and quantity of data rise, it can be difficult to establish whether the data is of sufficient quality to provide reliable insights.

4. Choosing the Best Big Data Tools:
Fortunately, when a company decides to begin dealing with data, there is no shortage of tools to assist them. At the same time, the abundance of possibilities is a challenge. Big data software comes in a variety of flavors, and their capabilities frequently overlap. How do you know you're picking the correct big data tools?

source:Contributor, S. M.-. G. (2022, October 10). Top 8 challenges of big data and how to solve them. Capterra. https://www.capterra.com/resources/challenges-of-big-data/
